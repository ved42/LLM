{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the necessary library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "api_key = os.getenv('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'D:\\\\LLM\\\\Project1\\\\PDF\\\\An_Efficient_Machine_Learning_Approach_Analysis_of_Supervised_Machine_Learning_M.pdf', 'page': 0}, page_content='An Efficient Machine Learning Approach: Analysis \\nof Supervised Machine Learning Methods to Forecast \\nthe Diamond Price\\nMd Shaik Amzad Basha \\nGITAM School of Business. \\nGandhi Institute of Technology and \\nManagement (Deemed to be University) \\nBengaluru, India \\namjuamjad66@gmail.com Peerzadah Mohammad Oveis  \\nGITAM School of Business. \\nGandhi Institute of Technology and \\nManagement (Deemed to be University) \\nBengaluru, India\\npeerzadahmohammad@gmail.com C Prabavathi \\nDepartment of Professional Studies \\nChrist (Deemed to be University) \\nBengaluru, India \\nprabavathidps@gmail.com\\nMacherla Bhagya Lakshmi \\nDepartment of Professional Studies \\nChrist (Deemed to be University)\\nBengaluru, India \\nbhagyamacherladps@gmail.com M Martha Sucharitha \\nDepartment of Professional Studies \\nChrist (Deemed to be University)\\nBengaluru, India \\nmarthasucharitha@gmail.com \\nAbstract — Diamond, a found natural process compound of \\ncarbon, is one of the hardest and most immensely expensive \\nmaterial known to men, especial ly more to women. Investments \\nin expensive gems like diamonds are in significant demand. The \\nrate of a diamond, nevertheless, is not as easily calculated as the \\nvalue of either gold or platinum since so many factors must be \\ntaken into account. Because there is such a broad range of \\ndiamond dimensions and qualities; as a result, being able to \\nmake reliable price predictions is crucial for the diamond \\nindustry. Although, making accurate predictions is challenging. \\nIn this study, we implemented multiple machine learning \\ntechniques employed to the challenge of diamond price \\nforecasting’s such as Linear  Regression, Random Forest, \\nDecision Tree Random Forest, Cat-Boost Regressor and XGB \\nRegressor. This article\\'s goal is to develop an accurate model for \\nestimating diamond prices based on its characteristics such as \\nweighting factor, cut grade, and dimensions. We compared the \\nsum of estimated values and test values of predicted values with \\noverestimated, underestimated and exact estimations. We applied \\ncross-validation to calculate how much the model deviates from \\nthe actual when faced with a difference between the training set \\nand the test set. We predicted values side by side. We performed \\na comparative analysis of supervised machine learning models \\nwith other models to evaluate the model accuracy and \\nperformance metrics. The Study\\'s experimental findings show \\nthat out of all the supervised machine learning models, Random \\nForest performs well with R2score and Low RMSE and MAE \\nvalues and CV Score. \\nKeywords — Estimated values, Machine learning, Diamonds, \\nRegression Models and Cross validation \\nI. I NTRODUCTION \\n In terms of market value, diamonds are by far the most \\nsought-after gem kind. Diamonds demand a price tag that is \\ntens of times higher than that of any other gemstones. \\nDiamond\\'s optical characteristics, or how it interacts with light, \\ncontribute to its widespread acclaim. Diamonds are popular for a variety of reasons outside their obvious toughness and \\nlongevity, including the fashion industry, cultural significance, \\nand producers\\' persistent advertising. When it comes to jewelry,\\ndiamonds are at the very top. the world\\'s most expensive items, \\nas it possesses the astounding capability of spreading light \\neverywhere. The worth of a diamond is determined by various \\nfactors, including its shape, cut, inclusions (impurities), and \\nweight in carats. Diamonds have various industrial applications \\ndue to their superior slicing, buffing, and piercing capabilities. \\nThe tremendous worth of gemstones has ensured their \\ncontinued worldwide trade for millennia. Faceted diamonds are \\nevaluated based on their color, cut, clarity, and carat weight to \\nassess their overall quality. In the 1950s, the Gemological \\nInstitute of America created a standardized technique for \\nevaluating diamonds quality known as the \"4Cs of Diamond \\nQuality.\" [1] \\nAs a rule, the weight of materials such as platinum and'),\n",
       " Document(metadata={'source': 'D:\\\\LLM\\\\Project1\\\\PDF\\\\An_Efficient_Machine_Learning_Approach_Analysis_of_Supervised_Machine_Learning_M.pdf', 'page': 0}, page_content='Institute of America created a standardized technique for \\nevaluating diamonds quality known as the \"4Cs of Diamond \\nQuality.\" [1] \\nAs a rule, the weight of materials such as platinum and \\ngold are used to determine their worth, but the pricing of \\ndiamond depends on a number of other aspects as well. The \\ncarat, the cut, and a host of others are all relevant. Since \\ndiamonds are so expensive, even  a little shift in these variables \\nwould have a major impact on the diamond\\'s final price. \\nDiamonds, like any other commodity, go through a \\nproduction process that adds value at each stage until they \\nreach the retail shops. The cost of a polished diamond is \\nestablished after the raw diamonds has been mined and cut. \\nWhen a diamond is destined for use as an ornament, it goes \\nthrough a number of procedures to enhance its appearance and \\nchange it into a stunning adornment human practice of \\nadorning oneself. The initial cost of a diamond depends on its \\nrarity, size, and the time and effort required to polish, cut, and \\nmine them. The right price depends on several factors, not just \\none from the gems. The four Cs  refer to the color, clarity, \\ncarat, cut, and presentation of the gemstone. dimensions like \\ndepth, width, table. In particular, color, cut, clarity, and carat 2023 International Conference for Advancement in Technology (ICONAT) \\nGoa, India. Jan 24-26, 2023\\n978-1-6654-7517-4/23/$31.00 ©2023 IEEE 12023 International Conference for Advancement in Technology (ICONAT) | 978-1-6654-7517-4/23/$31.00 ©2023 IEEE | DOI: 10.1109/ICONAT57137.2023.10080618\\nAuthorized licensed use limited to: Indira Gandhi Centre for Atomic Research (IGCAR). Downloaded on October 27,2023 at 09:54:48 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'source': 'D:\\\\LLM\\\\Project1\\\\PDF\\\\An_Efficient_Machine_Learning_Approach_Analysis_of_Supervised_Machine_Learning_M.pdf', 'page': 1}, page_content='are four of the most important characteristics. The two most \\nimportant aspects of a product are its weight and its color. The four characteristics of a diamond that most affect its price are synonymous with the \"4Cs of Diamond Quality.\"[2].\\nThere were several experts we investigated who developed \\nand tested a wide range of machine learning models that learn the optimal solution to a given problem build a model to \\nforecast prices.  pricing is a continual, changeable goal variable. \\nwe determined that supervised regression algorithms should be employed. Section I introduced, the study is divided into three sections: sec. II reviews the literature of related works, while Sec. III provides Research Methodology. The proposed \\nmechanism for determining diamond prices is outlined in sec III machine learning relies on the use of supervised regression methods, and this article discusses both \\ntypes of methods as well as the dataset used to train them. The \\nfindings and evaluation of regression algorithms based on measured technical specifications  are discussed in Sec IV. In \\nSec V, we concluded the research that has to be done to compare the models for accuracy and prediction of diamond price. \\nII. L\\nITERATURE REVIEW\\nG. Sharma et. al [3] In their study they explained the \\ncomparative analysis of machine learning techniques that used to predict the diamond price and they evaluated best accuracy model based on the findings of their research study, Experimentation and analysis lead us to the conclusion that diamond pricing evaluated using supervised learning approaches including Ada-Boost, Gradient-Boosting, linear regression, decision tree, Elastic-Net, ridge, lasso regression, and the random forest approach. Their findings that accuracy \\nof the Random Forest Regression Algorithm is 97%. Its strong \\ncapability to calculate continuously numerical values allows it to provide such a high level of precision. A. C. Pandey et. al [4]findings of their research they have done the comparative analysis between various ensemble models and regressors. \\nState-of-the-art methods like gradient boosting, ada boost, and \\nbagging have been contrasted to the suggested model\\'s performances. All of the approaches\\' conclusions have been shown as regression model feature choices. The performance of current ensemble models improved by adding the feature selection and preprocessing stages, as shown by a comparison of the results achieved using ensemble regressors and feature choices with regression models. that collection of data used for instruction. Chu S [5] article serves as an example of how to construct a linear regression model. It is demonstrated that by employing a exponential regression model, one sidestep the appearance of a negative intercept, which would be counter-intuitive. Since these regression methods are naturally linear, typical linear regression techniques used to estimate them once the data has been transformed appropriately. In this Chu s [6]\\narticle, they explained about the comparative analysis of \\nregression models are linear in nature. The most effective and precise model is found in this work  by S. A. Fitriani et al [7].\\nThe k-Nearest Neighbor (kNN) and Least Absolute Shrinkage and Selection Operator (LASSO) models were created to predict diamond prices (LASSO). In order to achieve \\nmaximum precision, we carefully choose features by weighing \\nthe k value from k-NN against the alpha value from LASSO. Through a comparison of RMSE and R2, they found that the k-\\nNN method outperforms the LASSO approach. k-NN produces the lowest RMSE value (926.07) and the greatest R2 value (0.9066), or 90.66 percent. A variety of machine learning methods, including Liner regression, random forest, and Ensemble, were compared in their W. Alsuraihi [8] work to aid in the prediction of diamond price. The use of polynomial regression, forest regression, gradient descent, and a system of \\nneurons. Following exhaustive model training, accuracy testing,'),\n",
       " Document(metadata={'source': 'D:\\\\LLM\\\\Project1\\\\PDF\\\\An_Efficient_Machine_Learning_Approach_Analysis_of_Supervised_Machine_Learning_M.pdf', 'page': 1}, page_content=\"neurons. Following exhaustive model training, accuracy testing, \\nand data analysis, random forest regression was shown to be the most effective model. In spit e of the noise, they found that \\nrandom forest produced the best outcome. This is why we suggest that you create an ensemble model or use a random forest. By merging many models, ensemble learning can increase machine learning outcomes and address issues with \\n(bagging), bias (boosting). \\nEvaluation of the three linear algorithms Using a \\ncombination of regression, neural network, and M5P, they investigated estimating the value of diamonds price Pena et. al [9]. Within the scope of this study, they placed a procedure for reducing the number of dimensions and the issue of similarity between elements of the dataset. In a number of cases, this in general, the accuracy drops when there are \\nassociated characteristics in the dataset. the model itself A good illustration of this seen in the collection of data, where the Width are extremely important characteristics. connected to one another As a result, the model's accuracy can be improved by excluding linked characteristics. Using the diamonds dataset, the M5P model's accuracy is with a score of 98.7 percent on the training data, we were able to for dimensionality reduction, \\nthe M5P method is quite precise. reached a new high of \\n99.03%. Based on the research presented in their study, conclusion that the M5P algorithm is the most appropriate.\\nUsing a custom-built algorithm, we are able to anticipate future \\ndataset by means of reducing its dimensions. \\nBy analyzing the results of many regression methods [10]. \\nusing the dataset, we were able to determine which ones would be most useful for predicting diamond prices in the future. We analyze cross validation score as a result, we have examined various techniques to identify the most appropriate algorithm for creating and forecasting the value of diamonds.\\nIII. P\\nROPOSED METHODOLOGY\\n The whole execution of diamond price forecasting is \\nbroken down into several sub-steps, such as preprocessing the dataset, normalization of the data, removing duplicate values, missing numbers, label encoding as categorical and numerical and train the supervised machine learning approaches to determine the optimal methods for the price forecasting based on a number of metrics, including R2 score, root mean square error (RMSE), and mean absolu te error (MAE). The technique \\nwe provided and the procedures we used to put the models into \\naction are depicted in Fig. 1. As shown in the workflow \\nsequence of Figure 1, the data is normalized and cleaned before \\nit is utilized for data to train. The dataset is divided into two parts: the train data, and uutilized to create the models, and the test data, which is utilized to assess the quality of the \\nalgorithms and calibrate the important performance \\ncharacteristics. After extensiv e training and testing, the \\n2\\nAuthorized licensed use limited to: Indira Gandhi Centre for Atomic Research (IGCAR). Downloaded on October 27,2023 at 09:54:48 UTC from IEEE Xplore.  Restrictions apply.\"),\n",
       " Document(metadata={'source': 'D:\\\\LLM\\\\Project1\\\\PDF\\\\An_Efficient_Machine_Learning_Approach_Analysis_of_Supervised_Machine_Learning_M.pdf', 'page': 2}, page_content=\"parameters for all of the algorithms' process variables are \\ndetermined. The regression models are then used to predict diamond prices using these statistics.  \\nFig. 1. Workflow of Regression Models \\nA. Dataset  \\nKaggle is a platform for sharing and discussing machine \\nlearning and data sciences te chniques. For the purpose of \\ntraining ML models, it provides easy access to hundreds of datasets. In addition to sharing their models, users could \\ndevelop them and tests with other tools. Specifically, we have \\ntrained our supervised machine learning models using Diamond file collection [11]. \\nB. Data Description \\nThere are a total of 53,940 distinct entries in the diamond \\ncollection dataset. Attributes of the given dataset depicted in Table 1. The best-guess price of diamonds predicted with the use of the data's qualities. The 4Cs of the diamond refer to these four characteristics and are commonly used terminology\\namong diamond and jewelry experts.  \\nFig. 2. Diamond Dimensions \\nTABLE I. D ATA SET FEATURES\\nFeatures Range\\nCut (Fair, Good, Very \\nGood, Premium, Ideal)\\nColor J (worst) - D (best)\\nClarity (I1 (worst), SI2, SI1, \\nVS2, VS1, VVS2, \\nVVS1, IF (best))\\nCarat 0.2 - 5.01 ct\\nDepth (0 - 31.8) mm\\nTable (43 - 95) mm\\nPrice ($326--$18,823)\\nX(Length) (0 - 10.74) mm\\nY(Width) (0 - 58.9) mm\\nZ (Depth) (0 - 31.8) mmFrom Table 1 shows from All the diamonds in the data \\ncollection the vary in carat weig ht from 0.21 to 5.01 kg. There \\nare five possible levels of excellence for the cut: excellent, \\npremium, decent, and fair. Diamonds could be any color, from \\nthe poorest (J) to the greatest (D ), depending on the scale. \\nThere are eight distinct values for the clarity trait, ranging \\nfrom the lowest possible clarity (I1) to the highest possible \\nclarity (IF). Values for depth, ta ble, price, and x, y, and z can \\nrange from integers to float points. Table, Width, and Depth of \\na Diamond are Measured as Depicted in Fig.2. Since the cost \\nof a diamond is based largely on these four factors —carat \\nweight, diamond cut quality, color, and clarity —they deserve \\nspecial attention. \\nC. Data Pre-Processing \\nThe dataset is preprocessed an d optimized so that it used \\neffectively during training, as il lustrated in the flowchart of \\nFig. 1. There are two distinct subsets of the dataset: the training data, which is used to develop the models, and the \\ntesting data, which is used to evaluate the developed models \\nand determine the values of the relevant performance parameters. All of the models' performance parameter values \\nare acquired after training and testing. These numbers are \\nutilized to determine the optimum regression models, which \\nare then used for diamond price forecasting. The supervised \\nregression models are fed these parameters, and an USD-based forecast is produced. We employed graphical \\nrepresentations to examine the distributions of the dataset's \\nthree categorical variables: cut, clarity, and color. Across all three categories, we find s ufficient data to enable our \\nalgorithm to understand Fig 3,4,5. Because there would be less \\nreference points for each category if the lines were skewed, \\nthe machine learning system would have difficulties \\nin learning. \\nFig. 3. Distribution of x,y,z \\nFig. 4. Distribution of Cut, Clarity and color DatasetData Pre -\\nProcessingFeature Selection\\nLabel Encoding\\nand Feature ScalingBuild a Pipeline\\nfor ml modelsTraining\\nallml\\nmodels\\nTesting\\nmode lsComparing \\nthemodel Visualization\\n3\\nAuthorized licensed use limited to: Indira Gandhi Centre for Atomic Research (IGCAR). Downloaded on October 27,2023 at 09:54:48 UTC from IEEE Xplore.  Restrictions apply.\"),\n",
       " Document(metadata={'source': 'D:\\\\LLM\\\\Project1\\\\PDF\\\\An_Efficient_Machine_Learning_Approach_Analysis_of_Supervised_Machine_Learning_M.pdf', 'page': 3}, page_content=\"Fig. 5. Distribution of Depth, Carat and Price \\nWe use plots to learn about  the continuously factors' \\ndistribution, and a slightly skewed bell curve is what we want \\nto see most of the time. The va riables carat, depth, and table \\nare skewed; however, the parameters x, y, and z are skewed, therefore we need to eliminate extreme values to get them \\nnearer to the bell - shaped curve. \\nD. Label Encoding \\n Various pre-processing techniques, such as Labeling \\nEncode, are applied to the real dataset prior to training the \\nvarious ml models using it, in order to optimize the dataset for \\ntraining. To improve training efficiency for ml algorithms, we \\nconduct Labeling Encode on the dataset to encode its categorical features as numeric values. \\nTABLE II. L ABEL ENCODING \\nCarat Cut Color Clarity Depth Price Table x y z\\n0.23 2 1 3 61.5 55.0 326 3.95 3.98 2.43\\n0.21 3 1 2 59.8 61.0 326 3.89 3.84 2.31\\n0.23 1 1 4 56.9 65.0 327 4.05 4.07 2.31\\n0.29 3 5 5 62.4 58.0 334 4.20 4.23 2.63\\n0.29 3 5 5 62.4 58.0 334 4.20 4.23 2.63\\nUsing Labeling Encode, we transform the dataset's \\nproperties from their original alphanumeric form into meaningful numerical identifiers in Table 1. These elements are labeled from 0 to n, where n is the number of categories. Categorical variables in the diamond dataset are encoded using Label Encoding.  \\nThese include cut, color, and clarity. In addition, 70% of \\nthe records are kept for training and 30% are kept for testing, thus dividing the dataset in half. Finally, we maintain a random state of '42' to ensure that we are drawing from the same pool of potential samples throughout our investigation.\\nE. Correlation of Features \\nThe concept of correlation is useful for understanding the \\nconnections between different variables. These factors could be elements of the input data that have been employed in previous attempts to predict the outcome variable. With the use of \\ncorrelation, a statistical method, we would learn about the \\nconnections between two variables and how they develop over time. Here, we applied a heatmap to visually show the correlation matrix and locate highly correlated components.  Fig. 6. Correlation Matrix \\nFrom the Fig 6 We find a strong relationship between the \\ncharacteristics x, y, z, as well as carat and the dependent parameter cost. We can easily consider these characteristics while training our models. The characteristics 'depth,' 'cut,' and 'table' have minimal correlation and might be removed. Though the number of characteristics in the collection is small, we have \\nopted to retain it. After carefully examination we analyzed the \\noutliners before and after removal of outliners from the fig 7 and 8.\\nFig. 7. Outliners \\nAn outlier is a data point in a dataset whose value is \\nextremely out of line with the rest of the data. Data \\ninconsistency or scientific or human mistake during data \\ncollection are two possible causes. Statistical testing and model training could both suffer from the presence of outliers. Some of the various methods available for finding outliers include the Z-score, the IQR (Interquartile Range) approach, and DBSCAN clustering. After examining our data using boxplots, we used the IQR technique to eradicate any outliers. These points serve as a brief overview of the method: The first step is to determine the IQR for each column. To solve problem 2, find the constant that corresponds to 1.5 * (IQR). Third, multiply this constant by the third quartile, and then discard \\nany information that is higher th an this threshold. Specifically, \\ntake the first quartile and subtract the constant, and then get rid of any and all data points that are below the new lower limit. \\nFigure 4 shows the boxplots of the features, which we use to \\nidentify outliers in the dataset and exclude using the \\n4\\nAuthorized licensed use limited to: Indira Gandhi Centre for Atomic Research (IGCAR). Downloaded on October 27,2023 at 09:54:48 UTC from IEEE Xplore.  Restrictions apply.\"),\n",
       " Document(metadata={'source': 'D:\\\\LLM\\\\Project1\\\\PDF\\\\An_Efficient_Machine_Learning_Approach_Analysis_of_Supervised_Machine_Learning_M.pdf', 'page': 4}, page_content=\"interquartile range (IQR) method before continuing to train our \\nmodel. Figure 7 displays the boxplots that resulted when the data was cleared. \\nFig. 8. Removal of outliners \\nF. Linear Regression:  \\nTo forecast target value for i nputs that are not included in \\nthe data set we have, we appl y a technique called linear \\nregression, which involves creati ng a line that best matches \\nthe measured values visible on the plot. A more \\nstraightforward Linear regression model makes it less of a \\nchallenge to explain how the method fits and what the findings demonstrates. By using regre ssion analysis, we could learn \\nhow significant associations are among both predictors. It is \\npossible for us to predict a distinct factor for each attribute, but we also have the capability of obtaining a spectrum of \\nfactors along with a threshold of certainty that the value is in \\nthat scope. \\nG. Decision trees \\nStatistical tests could be quic kly and easily used to verify \\nthe algorithm's accuracy. Because of this, Decision Trees \\ncould be considered a trustwor thy structure.  Decision trees \\nare useful for understanding the likelihood of various outcomes by breaking down a huge dataset into subgroups that \\ninclude cases with comparable values [12]. Targeted \\nchangeable classes or values co uld well be predicted using \\ndecision trees. There is less work involved in data \\npreprocessing for decision trees than for other approaches. A \\ndecision tree could be constructed without first normalizing the data. The data in a decision tree does not need to be scaled \\nin any way. Decision tree construction is not significantly \\nimpacted by the presence or absence of missing data [13]. \\nH. Random Forest \\nThe random forest approach utilizes several decision trees \\nto arrive at a categorization. It uses bagging and characteristic \\npseudo random to create a stat istically independent forest of \\ntrees whose collective prediction is more accurate than that of \\nany classification algorithm. Util izing base classifiers from the \\ntrain data and a random characteristic choice in graph \\nformation, a Random Forest is a collection of unpruned \\ncategorization or regressed networks [14]. I. XG Boost \\nBoth a linear regression modeling solution and a tree \\nretraining method are available inside XG Boost. The ability \\nto do several calculations simultaneously on a particular \\nmachine is what makes it so quick [15]. Cross-validation could be conducted, and key var iables can be identified using \\nits supplementary characteristics. XG Boost excels on \\norganized, medium-sized sets of data with a reasonable \\nnumber of characteristics and s ubcategories. This strategy is \\nhighly recommended since XG Bo ost excels at regression and \\nclassification which account for most real concerns. The prototype capabilities it provides are delivered quickly \\nbecause of the efficiency of its boosting method. \\nJ. CAT Boost \\n In Cat boost, we used parame ters to enhance circumstances \\nbased on measurable criteria. E rror probability monitoring \\ncould be done rapidly as well. Cat Boost [16] allows for the \\nincorporation of non-numerical elements, which reduces the \\nneed for processing of the data and yields better training \\nresults. \\nIV. R\\nESULT ANALYSIS\\nIn order to determine which approach was accurate, we \\ntrained machine learning methods on the same data and \\ncompared their individual parameters. In order to attain the \\nrequired effectiveness of the approach, Cross-Validation, a variable selection approach, is us ed to swap out subsets of the \\ntraining data repeatedly and ar bitrarily, such as verification \\nand analysis, in required to trai ning and validate the algorithm. \\nIt is calculated by averaging the total disparity between \\nobserved and forecasted outcomes . To put it another way, the \\nMAE indicates that the model's predictions are, on average, off by a larger margin than the true value. For any given \\nmodel, the lower the MAE, the more accurate the forecast.\"),\n",
       " Document(metadata={'source': 'D:\\\\LLM\\\\Project1\\\\PDF\\\\An_Efficient_Machine_Learning_Approach_Analysis_of_Supervised_Machine_Learning_M.pdf', 'page': 4}, page_content=\"MAE indicates that the model's predictions are, on average, off by a larger margin than the true value. For any given \\nmodel, the lower the MAE, the more accurate the forecast. \\nThe average variance between the observed values and the projected values is expres sed as a squared error. The \\nadvantage of MSE is that it allows us to perform squared, \\nwhich prevents the elimination  of negative terms. Root-mean-\\nsquared error (RMSE) is calcul ated by averaging the squared \\ndisparity across the anticipated and observed results. Since \\nRMSE squared the mistakes befo re averaging them, it is a \\nmore effective productivity statis tic, especially for penalizing \\nvery egregious mistakes. Th e Cross validation, R2, MAE, \\nRMSE scores of all the models  employed for forecasting of \\ndiamond price on the same data are shown in a comparison \\nseen in Table. III, IV, V \\nTABLE III. ANALYSIS\\nMachine learning \\nModelsCV SCORE R2 score MAE RMSE\\nLinear Regression 1344.79 0.9731761 311.7599 648.69748\\nDecision Tree 49.9695 0.9999176 2.986569 35.944145\\nRandom Forest 35.9515 0.9999508 3.236079 27.759516\\nXGB Regressor 205.509 0.9973517 126.6803 203.82597\\nCat Boost 70.157 0.9997169 39.14737 66.637138\\n5\\nAuthorized licensed use limited to: Indira Gandhi Centre for Atomic Research (IGCAR). Downloaded on October 27,2023 at 09:54:48 UTC from IEEE Xplore.  Restrictions apply.\"),\n",
       " Document(metadata={'source': 'D:\\\\LLM\\\\Project1\\\\PDF\\\\An_Efficient_Machine_Learning_Approach_Analysis_of_Supervised_Machine_Learning_M.pdf', 'page': 5}, page_content='Table III shows that Random Forest has the greatest R2 \\nscore (0.9999). In terms of Cross validation score 35.9515 \\nlowest observed, and RMSE value is lowest of all models. In\\nterms of accuracy, other models  like XG-Boost, K-Nearest, \\nCat-Boost are all quite close. We have utilized Random Forest \\nto estimate diamond prices ba sed on user input since it \\noutperforms competing methods \\nTABLE IV. E STIMATIONS\\nMachine\\nlearning \\nModelsOver\\nEstimatedUnder \\nEstimatedExact \\nEstimationSum of \\nEstimated \\nValuesTest\\nValues\\nLinear 6976 6375 126 13477 13477\\nDecision \\nTree2013 2592 8872 13477 13477\\nRandom \\nForest1182 6291 6004 13477 13477\\nXGB\\nRegressor7366 6072 39 13477 13477\\nCat Boost 6512 6822 143 13477 13477\\nTable IV shows Decision Trees model predicted exact \\nestimation of 8872 values. Under estimated, overestimated \\nvalues of 2013 and 2592 \\nTABLE V. P REDICTED VALUES\\nLinear\\nRegression XGBDecision\\nTreeRandom \\nForest CAT Boost\\ny_test predicted predicted predicted predicted predicted\\n6426 6137 6639.794 6426 6425.99 6458.79957\\n2771 2919.2 2809.7625 2770 2770.61 2785.255335\\n3903 4006.2 3996.2593 3900 3901.63 3993.801772\\n3678 3409 3745.6956 3678 3677.08 3633.597001\\n660 742 710.1307 660 660.03 711.6079873\\n Table V shows that out of 100 values decision tree predicted prices exact closely and other models like Linear Regression have low Predicted prices. Random forest \\npredicted prices closely a nd XGB, Cat boost predicted \\noverpriced values \\nV. C\\nONCLUSIONS\\nIn this investigation, researchers employed machine \\nlearning strategies to anticipate future diamond prices. Following are some findings from this study: We trained, \\ntested, and evaluated different machine learning approaches \\n(Linear Regression, Random Forest, Decision Tree, Cat Boost Regressor, and XGB Regressor), after comparing the \\nperformance of different algorithms for predicting diamond \\nprices random forest approach outperforms. The Experimental findings shows that Random Forest Regression predicted \\naccurate values and high R2 score, low RMSE and MAE values.  Decision Tree outperforms predicted values Side by \\nside prices. From the literatu re review we found our model \\nperformed better results. Out of all Performance metrics \\nRandom Forest low RMSE valu e, and next close value \\nachieved by Decision Tree.  \\nR\\nEFERENCES\\n[1] Diamond-The most popular ge mstone\", [online] Available: \\nhttps://geology.com/minerals/diamond.shtml.  \\n[2] S. A. Fitriani, Y. Astuti and I. R.  Wulandari, \"Least Absolute Shrinkage \\nand Selection Operator (LASSO) and k-Nearest Neighbors (k-NN) \\nAlgorithm Analysis Based on Feature Selection for Diamond Price Prediction,\" 2021 International Se minar on Machine Learning, \\nOptimization, and Data Science, 2022, pp. 135-139\\n[3] G. Sharma, V. Tripathi, M. Ma hajan and A. Kumar Srivastava, \\n\"Comparative Analysis of Supervised Models for Diamond Price Prediction,\" 2021 11th International Co nference on Cloud Computing, \\nData Science & Engineering (Confluence), 2021. \\n[4] A. C. Pandey, S. Misra and M.  Saxena, \"Gold and Diamond Price \\nPrediction Using Enhanced Ensemble Learning,\" 2019 Twelfth \\nInternational Conference on Cont emporary Computing (IC3), 2019. \\n[5] Singfat Chu (2001) Pricing the C ’s of Diamond Stones, Journal of \\nStatistics Education, 9: 2. \\n[6] Chu Singfat (1996) Diamond Ring Pricing Using Linear Regression, \\nJournal of Statistics Education. \\n[7] S. A. Fitriani, Y. Astuti and I. R.  Wulandari, \"Least Absolute Shrinkage \\nand Selection Operator (LASSO) and k-Nearest Neighbors (k-NN) Algorithm Analysis Based on Feat ure Selection for Diamond Price \\nPrediction,\" 2021 International Se minar on Machine Learning, \\nOptimization, and Data Science, 2022, pp. 135-139\\n[8] W. Alsuraihi, E. Al-Hazmi, K. Bawazeer and H. Alghamdi, \"Machine \\nLearning Algorithms for Diamond Price Prediction\", ACM International \\nConference Proceeding Se ries, pp. 150-154, 2020. \\n[9] Marmolejos, José M. Peña. “Imple menting Data Mining Methods to'),\n",
       " Document(metadata={'source': 'D:\\\\LLM\\\\Project1\\\\PDF\\\\An_Efficient_Machine_Learning_Approach_Analysis_of_Supervised_Machine_Learning_M.pdf', 'page': 5}, page_content='Learning Algorithms for Diamond Price Prediction\", ACM International \\nConference Proceeding Se ries, pp. 150-154, 2020. \\n[9] Marmolejos, José M. Peña. “Imple menting Data Mining Methods to \\nPredict Diamond Prices.” in 2018 conference of Data Science  \\n[10] Scikit-learn: Machine Learning in  Python, Pedregosa et al., JMLR 12, \\npp. 2825-2830, 2011. \\n[11] Diamonds dataset (2017). Ka ggle datasets repository. \\nhttps://www.kaggle.com/shivam2503/diamonds.  \\n[12] E. Gyimah and D. K. Dake, \"Using Decision Tree Classification \\nAlgorithm to Predict Learner Typologies for Project-Based Learning,\" 2019 International Conference on Com puting, Computational Modelling \\nand Applications (ICCMA), 2019, pp. 130-1304. \\n[13] X. Hu, Y. Yang, L. Chen and S.  Zhu, \"Research on a Customer Churn \\nCombination Prediction Model Based on Decision Tree and Neural Network,\" 2020 IEEE 5th International Conference on Cloud \\nComputing and Big Data Analytics  (ICCCBDA), 2020, pp. 129-132. \\n[14] H. V. Ramachandra, G. Balaraju, A.  Rajashekar and H. Patil, \"Machine \\nLearning Application for Black Friday Sales Prediction Framework,\" 2021 International Conference on Emerging Smart Computing and \\nInformatics (ESCI), 2021, pp. 57-61. \\n[15] H. V. Ramachandra, G. Balaraju, A.  Rajashekar and H. Patil, \"Machine \\nLearning Application for Black Friday Sales Prediction Framework,\" 2021 International Conference on Emerging Smart Computing and \\nInformatics (ESCI), 2021, pp. 57-61. \\n[16] X. Dou, \"Online Purchase Behavior Prediction and Analysis Using \\nEnsemble Learning,\" 2020 IEEE 5th International Conference on Cloud \\nComputing and Big Data Analytics  (ICCCBDA), 2020, pp. 532-53.\\n  \\n6\\nAuthorized licensed use limited to: Indira Gandhi Centre for Atomic Research (IGCAR). Downloaded on October 27,2023 at 09:54:48 UTC from IEEE Xplore.  Restrictions apply.')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load the pdf file\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"D:\\LLM\\Project1\\PDF\\An_Efficient_Machine_Learning_Approach_Analysis_of_Supervised_Machine_Learning_M.pdf\")\n",
    "docs = loader.load_and_split()\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'D:\\\\LLM\\\\Project1\\\\PDF\\\\An_Efficient_Machine_Learning_Approach_Analysis_of_Supervised_Machine_Learning_M.pdf', 'page': 0}, page_content=\"An Efficient Machine Learning Approach: Analysis \\nof Supervised Machine Learning Methods to Forecast \\nthe Diamond Price\\nMd Shaik Amzad Basha \\nGITAM School of Business. \\nGandhi Institute of Technology and \\nManagement (Deemed to be University) \\nBengaluru, India \\namjuamjad66@gmail.com Peerzadah Mohammad Oveis  \\nGITAM School of Business. \\nGandhi Institute of Technology and \\nManagement (Deemed to be University) \\nBengaluru, India\\npeerzadahmohammad@gmail.com C Prabavathi \\nDepartment of Professional Studies \\nChrist (Deemed to be University) \\nBengaluru, India \\nprabavathidps@gmail.com\\nMacherla Bhagya Lakshmi \\nDepartment of Professional Studies \\nChrist (Deemed to be University)\\nBengaluru, India \\nbhagyamacherladps@gmail.com M Martha Sucharitha \\nDepartment of Professional Studies \\nChrist (Deemed to be University)\\nBengaluru, India \\nmarthasucharitha@gmail.com \\nAbstract — Diamond, a found natural process compound of \\ncarbon, is one of the hardest and most immensely expensive \\nmaterial known to men, especial ly more to women. Investments \\nin expensive gems like diamonds are in significant demand. The \\nrate of a diamond, nevertheless, is not as easily calculated as the \\nvalue of either gold or platinum since so many factors must be \\ntaken into account. Because there is such a broad range of \\ndiamond dimensions and qualities; as a result, being able to \\nmake reliable price predictions is crucial for the diamond \\nindustry. Although, making accurate predictions is challenging. \\nIn this study, we implemented multiple machine learning \\ntechniques employed to the challenge of diamond price \\nforecasting’s such as Linear  Regression, Random Forest, \\nDecision Tree Random Forest, Cat-Boost Regressor and XGB \\nRegressor. This article's goal is to develop an accurate model for \\nestimating diamond prices based on its characteristics such as \\nweighting factor, cut grade, and dimensions. We compared the \\nsum of estimated values and test values of predicted values with\"),\n",
       " Document(metadata={'source': 'D:\\\\LLM\\\\Project1\\\\PDF\\\\An_Efficient_Machine_Learning_Approach_Analysis_of_Supervised_Machine_Learning_M.pdf', 'page': 0}, page_content=\"sum of estimated values and test values of predicted values with \\noverestimated, underestimated and exact estimations. We applied \\ncross-validation to calculate how much the model deviates from \\nthe actual when faced with a difference between the training set \\nand the test set. We predicted values side by side. We performed \\na comparative analysis of supervised machine learning models \\nwith other models to evaluate the model accuracy and \\nperformance metrics. The Study's experimental findings show \\nthat out of all the supervised machine learning models, Random \\nForest performs well with R2score and Low RMSE and MAE \\nvalues and CV Score. \\nKeywords — Estimated values, Machine learning, Diamonds, \\nRegression Models and Cross validation \\nI. I NTRODUCTION \\n In terms of market value, diamonds are by far the most \\nsought-after gem kind. Diamonds demand a price tag that is \\ntens of times higher than that of any other gemstones. \\nDiamond's optical characteristics, or how it interacts with light, \\ncontribute to its widespread acclaim. Diamonds are popular for a variety of reasons outside their obvious toughness and \\nlongevity, including the fashion industry, cultural significance, \\nand producers' persistent advertising. When it comes to jewelry,\\ndiamonds are at the very top. the world's most expensive items, \\nas it possesses the astounding capability of spreading light \\neverywhere. The worth of a diamond is determined by various \\nfactors, including its shape, cut, inclusions (impurities), and \\nweight in carats. Diamonds have various industrial applications \\ndue to their superior slicing, buffing, and piercing capabilities. \\nThe tremendous worth of gemstones has ensured their \\ncontinued worldwide trade for millennia. Faceted diamonds are \\nevaluated based on their color, cut, clarity, and carat weight to \\nassess their overall quality. In the 1950s, the Gemological \\nInstitute of America created a standardized technique for\"),\n",
       " Document(metadata={'source': 'D:\\\\LLM\\\\Project1\\\\PDF\\\\An_Efficient_Machine_Learning_Approach_Analysis_of_Supervised_Machine_Learning_M.pdf', 'page': 0}, page_content='Institute of America created a standardized technique for \\nevaluating diamonds quality known as the \"4Cs of Diamond \\nQuality.\" [1] \\nAs a rule, the weight of materials such as platinum and'),\n",
       " Document(metadata={'source': 'D:\\\\LLM\\\\Project1\\\\PDF\\\\An_Efficient_Machine_Learning_Approach_Analysis_of_Supervised_Machine_Learning_M.pdf', 'page': 0}, page_content='Institute of America created a standardized technique for \\nevaluating diamonds quality known as the \"4Cs of Diamond \\nQuality.\" [1] \\nAs a rule, the weight of materials such as platinum and \\ngold are used to determine their worth, but the pricing of \\ndiamond depends on a number of other aspects as well. The \\ncarat, the cut, and a host of others are all relevant. Since \\ndiamonds are so expensive, even  a little shift in these variables \\nwould have a major impact on the diamond\\'s final price. \\nDiamonds, like any other commodity, go through a \\nproduction process that adds value at each stage until they \\nreach the retail shops. The cost of a polished diamond is \\nestablished after the raw diamonds has been mined and cut. \\nWhen a diamond is destined for use as an ornament, it goes \\nthrough a number of procedures to enhance its appearance and \\nchange it into a stunning adornment human practice of \\nadorning oneself. The initial cost of a diamond depends on its \\nrarity, size, and the time and effort required to polish, cut, and \\nmine them. The right price depends on several factors, not just \\none from the gems. The four Cs  refer to the color, clarity, \\ncarat, cut, and presentation of the gemstone. dimensions like \\ndepth, width, table. In particular, color, cut, clarity, and carat 2023 International Conference for Advancement in Technology (ICONAT) \\nGoa, India. Jan 24-26, 2023\\n978-1-6654-7517-4/23/$31.00 ©2023 IEEE 12023 International Conference for Advancement in Technology (ICONAT) | 978-1-6654-7517-4/23/$31.00 ©2023 IEEE | DOI: 10.1109/ICONAT57137.2023.10080618\\nAuthorized licensed use limited to: Indira Gandhi Centre for Atomic Research (IGCAR). Downloaded on October 27,2023 at 09:54:48 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'source': 'D:\\\\LLM\\\\Project1\\\\PDF\\\\An_Efficient_Machine_Learning_Approach_Analysis_of_Supervised_Machine_Learning_M.pdf', 'page': 1}, page_content='are four of the most important characteristics. The two most \\nimportant aspects of a product are its weight and its color. The four characteristics of a diamond that most affect its price are synonymous with the \"4Cs of Diamond Quality.\"[2].\\nThere were several experts we investigated who developed \\nand tested a wide range of machine learning models that learn the optimal solution to a given problem build a model to \\nforecast prices.  pricing is a continual, changeable goal variable. \\nwe determined that supervised regression algorithms should be employed. Section I introduced, the study is divided into three sections: sec. II reviews the literature of related works, while Sec. III provides Research Methodology. The proposed \\nmechanism for determining diamond prices is outlined in sec III machine learning relies on the use of supervised regression methods, and this article discusses both \\ntypes of methods as well as the dataset used to train them. The \\nfindings and evaluation of regression algorithms based on measured technical specifications  are discussed in Sec IV. In \\nSec V, we concluded the research that has to be done to compare the models for accuracy and prediction of diamond price. \\nII. L\\nITERATURE REVIEW\\nG. Sharma et. al [3] In their study they explained the \\ncomparative analysis of machine learning techniques that used to predict the diamond price and they evaluated best accuracy model based on the findings of their research study, Experimentation and analysis lead us to the conclusion that diamond pricing evaluated using supervised learning approaches including Ada-Boost, Gradient-Boosting, linear regression, decision tree, Elastic-Net, ridge, lasso regression, and the random forest approach. Their findings that accuracy \\nof the Random Forest Regression Algorithm is 97%. Its strong'),\n",
       " Document(metadata={'source': 'D:\\\\LLM\\\\Project1\\\\PDF\\\\An_Efficient_Machine_Learning_Approach_Analysis_of_Supervised_Machine_Learning_M.pdf', 'page': 1}, page_content=\"of the Random Forest Regression Algorithm is 97%. Its strong \\ncapability to calculate continuously numerical values allows it to provide such a high level of precision. A. C. Pandey et. al [4]findings of their research they have done the comparative analysis between various ensemble models and regressors. \\nState-of-the-art methods like gradient boosting, ada boost, and \\nbagging have been contrasted to the suggested model's performances. All of the approaches' conclusions have been shown as regression model feature choices. The performance of current ensemble models improved by adding the feature selection and preprocessing stages, as shown by a comparison of the results achieved using ensemble regressors and feature choices with regression models. that collection of data used for instruction. Chu S [5] article serves as an example of how to construct a linear regression model. It is demonstrated that by employing a exponential regression model, one sidestep the appearance of a negative intercept, which would be counter-intuitive. Since these regression methods are naturally linear, typical linear regression techniques used to estimate them once the data has been transformed appropriately. In this Chu s [6]\\narticle, they explained about the comparative analysis of \\nregression models are linear in nature. The most effective and precise model is found in this work  by S. A. Fitriani et al [7].\\nThe k-Nearest Neighbor (kNN) and Least Absolute Shrinkage and Selection Operator (LASSO) models were created to predict diamond prices (LASSO). In order to achieve \\nmaximum precision, we carefully choose features by weighing \\nthe k value from k-NN against the alpha value from LASSO. Through a comparison of RMSE and R2, they found that the k-\"),\n",
       " Document(metadata={'source': 'D:\\\\LLM\\\\Project1\\\\PDF\\\\An_Efficient_Machine_Learning_Approach_Analysis_of_Supervised_Machine_Learning_M.pdf', 'page': 1}, page_content='NN method outperforms the LASSO approach. k-NN produces the lowest RMSE value (926.07) and the greatest R2 value (0.9066), or 90.66 percent. A variety of machine learning methods, including Liner regression, random forest, and Ensemble, were compared in their W. Alsuraihi [8] work to aid in the prediction of diamond price. The use of polynomial regression, forest regression, gradient descent, and a system of \\nneurons. Following exhaustive model training, accuracy testing,'),\n",
       " Document(metadata={'source': 'D:\\\\LLM\\\\Project1\\\\PDF\\\\An_Efficient_Machine_Learning_Approach_Analysis_of_Supervised_Machine_Learning_M.pdf', 'page': 1}, page_content=\"neurons. Following exhaustive model training, accuracy testing, \\nand data analysis, random forest regression was shown to be the most effective model. In spit e of the noise, they found that \\nrandom forest produced the best outcome. This is why we suggest that you create an ensemble model or use a random forest. By merging many models, ensemble learning can increase machine learning outcomes and address issues with \\n(bagging), bias (boosting). \\nEvaluation of the three linear algorithms Using a \\ncombination of regression, neural network, and M5P, they investigated estimating the value of diamonds price Pena et. al [9]. Within the scope of this study, they placed a procedure for reducing the number of dimensions and the issue of similarity between elements of the dataset. In a number of cases, this in general, the accuracy drops when there are \\nassociated characteristics in the dataset. the model itself A good illustration of this seen in the collection of data, where the Width are extremely important characteristics. connected to one another As a result, the model's accuracy can be improved by excluding linked characteristics. Using the diamonds dataset, the M5P model's accuracy is with a score of 98.7 percent on the training data, we were able to for dimensionality reduction, \\nthe M5P method is quite precise. reached a new high of \\n99.03%. Based on the research presented in their study, conclusion that the M5P algorithm is the most appropriate.\\nUsing a custom-built algorithm, we are able to anticipate future \\ndataset by means of reducing its dimensions. \\nBy analyzing the results of many regression methods [10]. \\nusing the dataset, we were able to determine which ones would be most useful for predicting diamond prices in the future. We analyze cross validation score as a result, we have examined various techniques to identify the most appropriate algorithm for creating and forecasting the value of diamonds.\\nIII. P\\nROPOSED METHODOLOGY\"),\n",
       " Document(metadata={'source': 'D:\\\\LLM\\\\Project1\\\\PDF\\\\An_Efficient_Machine_Learning_Approach_Analysis_of_Supervised_Machine_Learning_M.pdf', 'page': 1}, page_content='III. P\\nROPOSED METHODOLOGY\\n The whole execution of diamond price forecasting is \\nbroken down into several sub-steps, such as preprocessing the dataset, normalization of the data, removing duplicate values, missing numbers, label encoding as categorical and numerical and train the supervised machine learning approaches to determine the optimal methods for the price forecasting based on a number of metrics, including R2 score, root mean square error (RMSE), and mean absolu te error (MAE). The technique \\nwe provided and the procedures we used to put the models into \\naction are depicted in Fig. 1. As shown in the workflow \\nsequence of Figure 1, the data is normalized and cleaned before \\nit is utilized for data to train. The dataset is divided into two parts: the train data, and uutilized to create the models, and the test data, which is utilized to assess the quality of the \\nalgorithms and calibrate the important performance \\ncharacteristics. After extensiv e training and testing, the \\n2\\nAuthorized licensed use limited to: Indira Gandhi Centre for Atomic Research (IGCAR). Downloaded on October 27,2023 at 09:54:48 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'source': 'D:\\\\LLM\\\\Project1\\\\PDF\\\\An_Efficient_Machine_Learning_Approach_Analysis_of_Supervised_Machine_Learning_M.pdf', 'page': 2}, page_content=\"parameters for all of the algorithms' process variables are \\ndetermined. The regression models are then used to predict diamond prices using these statistics.  \\nFig. 1. Workflow of Regression Models \\nA. Dataset  \\nKaggle is a platform for sharing and discussing machine \\nlearning and data sciences te chniques. For the purpose of \\ntraining ML models, it provides easy access to hundreds of datasets. In addition to sharing their models, users could \\ndevelop them and tests with other tools. Specifically, we have \\ntrained our supervised machine learning models using Diamond file collection [11]. \\nB. Data Description \\nThere are a total of 53,940 distinct entries in the diamond \\ncollection dataset. Attributes of the given dataset depicted in Table 1. The best-guess price of diamonds predicted with the use of the data's qualities. The 4Cs of the diamond refer to these four characteristics and are commonly used terminology\\namong diamond and jewelry experts.  \\nFig. 2. Diamond Dimensions \\nTABLE I. D ATA SET FEATURES\\nFeatures Range\\nCut (Fair, Good, Very \\nGood, Premium, Ideal)\\nColor J (worst) - D (best)\\nClarity (I1 (worst), SI2, SI1, \\nVS2, VS1, VVS2, \\nVVS1, IF (best))\\nCarat 0.2 - 5.01 ct\\nDepth (0 - 31.8) mm\\nTable (43 - 95) mm\\nPrice ($326--$18,823)\\nX(Length) (0 - 10.74) mm\\nY(Width) (0 - 58.9) mm\\nZ (Depth) (0 - 31.8) mmFrom Table 1 shows from All the diamonds in the data \\ncollection the vary in carat weig ht from 0.21 to 5.01 kg. There \\nare five possible levels of excellence for the cut: excellent, \\npremium, decent, and fair. Diamonds could be any color, from \\nthe poorest (J) to the greatest (D ), depending on the scale. \\nThere are eight distinct values for the clarity trait, ranging \\nfrom the lowest possible clarity (I1) to the highest possible \\nclarity (IF). Values for depth, ta ble, price, and x, y, and z can \\nrange from integers to float points. Table, Width, and Depth of \\na Diamond are Measured as Depicted in Fig.2. Since the cost\"),\n",
       " Document(metadata={'source': 'D:\\\\LLM\\\\Project1\\\\PDF\\\\An_Efficient_Machine_Learning_Approach_Analysis_of_Supervised_Machine_Learning_M.pdf', 'page': 2}, page_content=\"a Diamond are Measured as Depicted in Fig.2. Since the cost \\nof a diamond is based largely on these four factors —carat \\nweight, diamond cut quality, color, and clarity —they deserve \\nspecial attention. \\nC. Data Pre-Processing \\nThe dataset is preprocessed an d optimized so that it used \\neffectively during training, as il lustrated in the flowchart of \\nFig. 1. There are two distinct subsets of the dataset: the training data, which is used to develop the models, and the \\ntesting data, which is used to evaluate the developed models \\nand determine the values of the relevant performance parameters. All of the models' performance parameter values \\nare acquired after training and testing. These numbers are \\nutilized to determine the optimum regression models, which \\nare then used for diamond price forecasting. The supervised \\nregression models are fed these parameters, and an USD-based forecast is produced. We employed graphical \\nrepresentations to examine the distributions of the dataset's \\nthree categorical variables: cut, clarity, and color. Across all three categories, we find s ufficient data to enable our \\nalgorithm to understand Fig 3,4,5. Because there would be less \\nreference points for each category if the lines were skewed, \\nthe machine learning system would have difficulties \\nin learning. \\nFig. 3. Distribution of x,y,z \\nFig. 4. Distribution of Cut, Clarity and color DatasetData Pre -\\nProcessingFeature Selection\\nLabel Encoding\\nand Feature ScalingBuild a Pipeline\\nfor ml modelsTraining\\nallml\\nmodels\\nTesting\\nmode lsComparing \\nthemodel Visualization\\n3\\nAuthorized licensed use limited to: Indira Gandhi Centre for Atomic Research (IGCAR). Downloaded on October 27,2023 at 09:54:48 UTC from IEEE Xplore.  Restrictions apply.\"),\n",
       " Document(metadata={'source': 'D:\\\\LLM\\\\Project1\\\\PDF\\\\An_Efficient_Machine_Learning_Approach_Analysis_of_Supervised_Machine_Learning_M.pdf', 'page': 3}, page_content=\"Fig. 5. Distribution of Depth, Carat and Price \\nWe use plots to learn about  the continuously factors' \\ndistribution, and a slightly skewed bell curve is what we want \\nto see most of the time. The va riables carat, depth, and table \\nare skewed; however, the parameters x, y, and z are skewed, therefore we need to eliminate extreme values to get them \\nnearer to the bell - shaped curve. \\nD. Label Encoding \\n Various pre-processing techniques, such as Labeling \\nEncode, are applied to the real dataset prior to training the \\nvarious ml models using it, in order to optimize the dataset for \\ntraining. To improve training efficiency for ml algorithms, we \\nconduct Labeling Encode on the dataset to encode its categorical features as numeric values. \\nTABLE II. L ABEL ENCODING \\nCarat Cut Color Clarity Depth Price Table x y z\\n0.23 2 1 3 61.5 55.0 326 3.95 3.98 2.43\\n0.21 3 1 2 59.8 61.0 326 3.89 3.84 2.31\\n0.23 1 1 4 56.9 65.0 327 4.05 4.07 2.31\\n0.29 3 5 5 62.4 58.0 334 4.20 4.23 2.63\\n0.29 3 5 5 62.4 58.0 334 4.20 4.23 2.63\\nUsing Labeling Encode, we transform the dataset's \\nproperties from their original alphanumeric form into meaningful numerical identifiers in Table 1. These elements are labeled from 0 to n, where n is the number of categories. Categorical variables in the diamond dataset are encoded using Label Encoding.  \\nThese include cut, color, and clarity. In addition, 70% of \\nthe records are kept for training and 30% are kept for testing, thus dividing the dataset in half. Finally, we maintain a random state of '42' to ensure that we are drawing from the same pool of potential samples throughout our investigation.\\nE. Correlation of Features \\nThe concept of correlation is useful for understanding the \\nconnections between different variables. These factors could be elements of the input data that have been employed in previous attempts to predict the outcome variable. With the use of \\ncorrelation, a statistical method, we would learn about the\"),\n",
       " Document(metadata={'source': 'D:\\\\LLM\\\\Project1\\\\PDF\\\\An_Efficient_Machine_Learning_Approach_Analysis_of_Supervised_Machine_Learning_M.pdf', 'page': 3}, page_content=\"correlation, a statistical method, we would learn about the \\nconnections between two variables and how they develop over time. Here, we applied a heatmap to visually show the correlation matrix and locate highly correlated components.  Fig. 6. Correlation Matrix \\nFrom the Fig 6 We find a strong relationship between the \\ncharacteristics x, y, z, as well as carat and the dependent parameter cost. We can easily consider these characteristics while training our models. The characteristics 'depth,' 'cut,' and 'table' have minimal correlation and might be removed. Though the number of characteristics in the collection is small, we have \\nopted to retain it. After carefully examination we analyzed the \\noutliners before and after removal of outliners from the fig 7 and 8.\\nFig. 7. Outliners \\nAn outlier is a data point in a dataset whose value is \\nextremely out of line with the rest of the data. Data \\ninconsistency or scientific or human mistake during data \\ncollection are two possible causes. Statistical testing and model training could both suffer from the presence of outliers. Some of the various methods available for finding outliers include the Z-score, the IQR (Interquartile Range) approach, and DBSCAN clustering. After examining our data using boxplots, we used the IQR technique to eradicate any outliers. These points serve as a brief overview of the method: The first step is to determine the IQR for each column. To solve problem 2, find the constant that corresponds to 1.5 * (IQR). Third, multiply this constant by the third quartile, and then discard \\nany information that is higher th an this threshold. Specifically, \\ntake the first quartile and subtract the constant, and then get rid of any and all data points that are below the new lower limit. \\nFigure 4 shows the boxplots of the features, which we use to \\nidentify outliers in the dataset and exclude using the \\n4\"),\n",
       " Document(metadata={'source': 'D:\\\\LLM\\\\Project1\\\\PDF\\\\An_Efficient_Machine_Learning_Approach_Analysis_of_Supervised_Machine_Learning_M.pdf', 'page': 3}, page_content='identify outliers in the dataset and exclude using the \\n4\\nAuthorized licensed use limited to: Indira Gandhi Centre for Atomic Research (IGCAR). Downloaded on October 27,2023 at 09:54:48 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'source': 'D:\\\\LLM\\\\Project1\\\\PDF\\\\An_Efficient_Machine_Learning_Approach_Analysis_of_Supervised_Machine_Learning_M.pdf', 'page': 4}, page_content=\"interquartile range (IQR) method before continuing to train our \\nmodel. Figure 7 displays the boxplots that resulted when the data was cleared. \\nFig. 8. Removal of outliners \\nF. Linear Regression:  \\nTo forecast target value for i nputs that are not included in \\nthe data set we have, we appl y a technique called linear \\nregression, which involves creati ng a line that best matches \\nthe measured values visible on the plot. A more \\nstraightforward Linear regression model makes it less of a \\nchallenge to explain how the method fits and what the findings demonstrates. By using regre ssion analysis, we could learn \\nhow significant associations are among both predictors. It is \\npossible for us to predict a distinct factor for each attribute, but we also have the capability of obtaining a spectrum of \\nfactors along with a threshold of certainty that the value is in \\nthat scope. \\nG. Decision trees \\nStatistical tests could be quic kly and easily used to verify \\nthe algorithm's accuracy. Because of this, Decision Trees \\ncould be considered a trustwor thy structure.  Decision trees \\nare useful for understanding the likelihood of various outcomes by breaking down a huge dataset into subgroups that \\ninclude cases with comparable values [12]. Targeted \\nchangeable classes or values co uld well be predicted using \\ndecision trees. There is less work involved in data \\npreprocessing for decision trees than for other approaches. A \\ndecision tree could be constructed without first normalizing the data. The data in a decision tree does not need to be scaled \\nin any way. Decision tree construction is not significantly \\nimpacted by the presence or absence of missing data [13]. \\nH. Random Forest \\nThe random forest approach utilizes several decision trees \\nto arrive at a categorization. It uses bagging and characteristic \\npseudo random to create a stat istically independent forest of \\ntrees whose collective prediction is more accurate than that of\"),\n",
       " Document(metadata={'source': 'D:\\\\LLM\\\\Project1\\\\PDF\\\\An_Efficient_Machine_Learning_Approach_Analysis_of_Supervised_Machine_Learning_M.pdf', 'page': 4}, page_content='trees whose collective prediction is more accurate than that of \\nany classification algorithm. Util izing base classifiers from the \\ntrain data and a random characteristic choice in graph \\nformation, a Random Forest is a collection of unpruned \\ncategorization or regressed networks [14]. I. XG Boost \\nBoth a linear regression modeling solution and a tree \\nretraining method are available inside XG Boost. The ability \\nto do several calculations simultaneously on a particular \\nmachine is what makes it so quick [15]. Cross-validation could be conducted, and key var iables can be identified using \\nits supplementary characteristics. XG Boost excels on \\norganized, medium-sized sets of data with a reasonable \\nnumber of characteristics and s ubcategories. This strategy is \\nhighly recommended since XG Bo ost excels at regression and \\nclassification which account for most real concerns. The prototype capabilities it provides are delivered quickly \\nbecause of the efficiency of its boosting method. \\nJ. CAT Boost \\n In Cat boost, we used parame ters to enhance circumstances \\nbased on measurable criteria. E rror probability monitoring \\ncould be done rapidly as well. Cat Boost [16] allows for the \\nincorporation of non-numerical elements, which reduces the \\nneed for processing of the data and yields better training \\nresults. \\nIV. R\\nESULT ANALYSIS\\nIn order to determine which approach was accurate, we \\ntrained machine learning methods on the same data and \\ncompared their individual parameters. In order to attain the \\nrequired effectiveness of the approach, Cross-Validation, a variable selection approach, is us ed to swap out subsets of the \\ntraining data repeatedly and ar bitrarily, such as verification \\nand analysis, in required to trai ning and validate the algorithm. \\nIt is calculated by averaging the total disparity between \\nobserved and forecasted outcomes . To put it another way, the'),\n",
       " Document(metadata={'source': 'D:\\\\LLM\\\\Project1\\\\PDF\\\\An_Efficient_Machine_Learning_Approach_Analysis_of_Supervised_Machine_Learning_M.pdf', 'page': 4}, page_content=\"observed and forecasted outcomes . To put it another way, the \\nMAE indicates that the model's predictions are, on average, off by a larger margin than the true value. For any given \\nmodel, the lower the MAE, the more accurate the forecast.\"),\n",
       " Document(metadata={'source': 'D:\\\\LLM\\\\Project1\\\\PDF\\\\An_Efficient_Machine_Learning_Approach_Analysis_of_Supervised_Machine_Learning_M.pdf', 'page': 4}, page_content=\"MAE indicates that the model's predictions are, on average, off by a larger margin than the true value. For any given \\nmodel, the lower the MAE, the more accurate the forecast. \\nThe average variance between the observed values and the projected values is expres sed as a squared error. The \\nadvantage of MSE is that it allows us to perform squared, \\nwhich prevents the elimination  of negative terms. Root-mean-\\nsquared error (RMSE) is calcul ated by averaging the squared \\ndisparity across the anticipated and observed results. Since \\nRMSE squared the mistakes befo re averaging them, it is a \\nmore effective productivity statis tic, especially for penalizing \\nvery egregious mistakes. Th e Cross validation, R2, MAE, \\nRMSE scores of all the models  employed for forecasting of \\ndiamond price on the same data are shown in a comparison \\nseen in Table. III, IV, V \\nTABLE III. ANALYSIS\\nMachine learning \\nModelsCV SCORE R2 score MAE RMSE\\nLinear Regression 1344.79 0.9731761 311.7599 648.69748\\nDecision Tree 49.9695 0.9999176 2.986569 35.944145\\nRandom Forest 35.9515 0.9999508 3.236079 27.759516\\nXGB Regressor 205.509 0.9973517 126.6803 203.82597\\nCat Boost 70.157 0.9997169 39.14737 66.637138\\n5\\nAuthorized licensed use limited to: Indira Gandhi Centre for Atomic Research (IGCAR). Downloaded on October 27,2023 at 09:54:48 UTC from IEEE Xplore.  Restrictions apply.\"),\n",
       " Document(metadata={'source': 'D:\\\\LLM\\\\Project1\\\\PDF\\\\An_Efficient_Machine_Learning_Approach_Analysis_of_Supervised_Machine_Learning_M.pdf', 'page': 5}, page_content='Table III shows that Random Forest has the greatest R2 \\nscore (0.9999). In terms of Cross validation score 35.9515 \\nlowest observed, and RMSE value is lowest of all models. In\\nterms of accuracy, other models  like XG-Boost, K-Nearest, \\nCat-Boost are all quite close. We have utilized Random Forest \\nto estimate diamond prices ba sed on user input since it \\noutperforms competing methods \\nTABLE IV. E STIMATIONS\\nMachine\\nlearning \\nModelsOver\\nEstimatedUnder \\nEstimatedExact \\nEstimationSum of \\nEstimated \\nValuesTest\\nValues\\nLinear 6976 6375 126 13477 13477\\nDecision \\nTree2013 2592 8872 13477 13477\\nRandom \\nForest1182 6291 6004 13477 13477\\nXGB\\nRegressor7366 6072 39 13477 13477\\nCat Boost 6512 6822 143 13477 13477\\nTable IV shows Decision Trees model predicted exact \\nestimation of 8872 values. Under estimated, overestimated \\nvalues of 2013 and 2592 \\nTABLE V. P REDICTED VALUES\\nLinear\\nRegression XGBDecision\\nTreeRandom \\nForest CAT Boost\\ny_test predicted predicted predicted predicted predicted\\n6426 6137 6639.794 6426 6425.99 6458.79957\\n2771 2919.2 2809.7625 2770 2770.61 2785.255335\\n3903 4006.2 3996.2593 3900 3901.63 3993.801772\\n3678 3409 3745.6956 3678 3677.08 3633.597001\\n660 742 710.1307 660 660.03 711.6079873\\n Table V shows that out of 100 values decision tree predicted prices exact closely and other models like Linear Regression have low Predicted prices. Random forest \\npredicted prices closely a nd XGB, Cat boost predicted \\noverpriced values \\nV. C\\nONCLUSIONS\\nIn this investigation, researchers employed machine \\nlearning strategies to anticipate future diamond prices. Following are some findings from this study: We trained, \\ntested, and evaluated different machine learning approaches \\n(Linear Regression, Random Forest, Decision Tree, Cat Boost Regressor, and XGB Regressor), after comparing the \\nperformance of different algorithms for predicting diamond \\nprices random forest approach outperforms. The Experimental findings shows that Random Forest Regression predicted'),\n",
       " Document(metadata={'source': 'D:\\\\LLM\\\\Project1\\\\PDF\\\\An_Efficient_Machine_Learning_Approach_Analysis_of_Supervised_Machine_Learning_M.pdf', 'page': 5}, page_content='accurate values and high R2 score, low RMSE and MAE values.  Decision Tree outperforms predicted values Side by \\nside prices. From the literatu re review we found our model \\nperformed better results. Out of all Performance metrics \\nRandom Forest low RMSE valu e, and next close value \\nachieved by Decision Tree.  \\nR\\nEFERENCES\\n[1] Diamond-The most popular ge mstone\", [online] Available: \\nhttps://geology.com/minerals/diamond.shtml.  \\n[2] S. A. Fitriani, Y. Astuti and I. R.  Wulandari, \"Least Absolute Shrinkage \\nand Selection Operator (LASSO) and k-Nearest Neighbors (k-NN) \\nAlgorithm Analysis Based on Feature Selection for Diamond Price Prediction,\" 2021 International Se minar on Machine Learning, \\nOptimization, and Data Science, 2022, pp. 135-139\\n[3] G. Sharma, V. Tripathi, M. Ma hajan and A. Kumar Srivastava, \\n\"Comparative Analysis of Supervised Models for Diamond Price Prediction,\" 2021 11th International Co nference on Cloud Computing, \\nData Science & Engineering (Confluence), 2021. \\n[4] A. C. Pandey, S. Misra and M.  Saxena, \"Gold and Diamond Price \\nPrediction Using Enhanced Ensemble Learning,\" 2019 Twelfth \\nInternational Conference on Cont emporary Computing (IC3), 2019. \\n[5] Singfat Chu (2001) Pricing the C ’s of Diamond Stones, Journal of \\nStatistics Education, 9: 2. \\n[6] Chu Singfat (1996) Diamond Ring Pricing Using Linear Regression, \\nJournal of Statistics Education. \\n[7] S. A. Fitriani, Y. Astuti and I. R.  Wulandari, \"Least Absolute Shrinkage \\nand Selection Operator (LASSO) and k-Nearest Neighbors (k-NN) Algorithm Analysis Based on Feat ure Selection for Diamond Price \\nPrediction,\" 2021 International Se minar on Machine Learning, \\nOptimization, and Data Science, 2022, pp. 135-139\\n[8] W. Alsuraihi, E. Al-Hazmi, K. Bawazeer and H. Alghamdi, \"Machine \\nLearning Algorithms for Diamond Price Prediction\", ACM International \\nConference Proceeding Se ries, pp. 150-154, 2020. \\n[9] Marmolejos, José M. Peña. “Imple menting Data Mining Methods to'),\n",
       " Document(metadata={'source': 'D:\\\\LLM\\\\Project1\\\\PDF\\\\An_Efficient_Machine_Learning_Approach_Analysis_of_Supervised_Machine_Learning_M.pdf', 'page': 5}, page_content='Learning Algorithms for Diamond Price Prediction\", ACM International \\nConference Proceeding Se ries, pp. 150-154, 2020. \\n[9] Marmolejos, José M. Peña. “Imple menting Data Mining Methods to \\nPredict Diamond Prices.” in 2018 conference of Data Science  \\n[10] Scikit-learn: Machine Learning in  Python, Pedregosa et al., JMLR 12, \\npp. 2825-2830, 2011. \\n[11] Diamonds dataset (2017). Ka ggle datasets repository. \\nhttps://www.kaggle.com/shivam2503/diamonds.  \\n[12] E. Gyimah and D. K. Dake, \"Using Decision Tree Classification \\nAlgorithm to Predict Learner Typologies for Project-Based Learning,\" 2019 International Conference on Com puting, Computational Modelling \\nand Applications (ICCMA), 2019, pp. 130-1304. \\n[13] X. Hu, Y. Yang, L. Chen and S.  Zhu, \"Research on a Customer Churn \\nCombination Prediction Model Based on Decision Tree and Neural Network,\" 2020 IEEE 5th International Conference on Cloud \\nComputing and Big Data Analytics  (ICCCBDA), 2020, pp. 129-132. \\n[14] H. V. Ramachandra, G. Balaraju, A.  Rajashekar and H. Patil, \"Machine \\nLearning Application for Black Friday Sales Prediction Framework,\" 2021 International Conference on Emerging Smart Computing and \\nInformatics (ESCI), 2021, pp. 57-61. \\n[15] H. V. Ramachandra, G. Balaraju, A.  Rajashekar and H. Patil, \"Machine \\nLearning Application for Black Friday Sales Prediction Framework,\" 2021 International Conference on Emerging Smart Computing and \\nInformatics (ESCI), 2021, pp. 57-61. \\n[16] X. Dou, \"Online Purchase Behavior Prediction and Analysis Using \\nEnsemble Learning,\" 2020 IEEE 5th International Conference on Cloud \\nComputing and Big Data Analytics  (ICCCBDA), 2020, pp. 532-53.\\n  \\n6\\nAuthorized licensed use limited to: Indira Gandhi Centre for Atomic Research (IGCAR). Downloaded on October 27,2023 at 09:54:48 UTC from IEEE Xplore.  Restrictions apply.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size =2000,chunk_overlap=100).split_documents(docs)\n",
    "text_splitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the huggingface Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\LLM\\Project1\\venv\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "d:\\LLM\\Project1\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = FAISS.from_documents(text_splitter,embeddings)\n",
    "retriver = store.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt=ChatPromptTemplate.from_template(\n",
    "\"\"\"\n",
    "Based on the {context} provided answer the query asked by the user in a best possible way.\n",
    "Example1- Question:\"What skill is necessary to become Data Scientist?\"\n",
    "Answer:\"SQL, Python, Machine Learning and concepts which help in future values predictions.\"\n",
    "Question:{input}\n",
    "Answer:\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intialize the LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "api_key=os.getenv(\"GROQ_API_KEY\")\n",
    "llm=ChatGroq(groq_api_key=api_key,model=\"llama3-8b-8192\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a chain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "combine_docs_model_with_prompt = create_stuff_documents_chain(llm,prompt)\n",
    "retrival_chain = create_retrieval_chain(retriver,combine_docs_model_with_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The article discusses the 4Cs of diamond quality, which are color, clarity, carat, and cut. These factors are used to evaluate the quality of a diamond and determine its price. The article also discusses the use of machine learning techniques to predict the price of a diamond based on these factors.\n",
      "\n",
      "The authors of the article trained several machine learning models, including linear regression, random forest, decision tree, cat-boost regressor, and XGB regressor, using a dataset of 53,940 diamond entries. The dataset included information on the cut, color, clarity, carat, and dimensions of each diamond, as well as its price.\n",
      "\n",
      "The authors found that the random forest model performed the best, with a mean absolute error (MAE) of $243.15. They also found that the carat and cut of the diamond were the most important factors in determining its price, followed by the color and clarity.\n",
      "\n",
      "The article concludes that machine learning techniques can be used to accurately predict the price of a diamond based on its characteristics. The authors suggest that this approach could be used to help jewelers and customers make informed decisions about the purchase of diamonds.\n",
      "\n",
      "As for the question \"What skill is necessary to become a Data Scientist?\", the answer is: SQL, Python, Machine Learning and concepts which help in future value predictions.\n",
      "\n",
      "The figure of diamond dimensions is not explicitly provided in the article, but it is mentioned that the table, width, and depth of a diamond are measured as depicted in Fig. 2.\n"
     ]
    }
   ],
   "source": [
    "response = retrival_chain.invoke({'input':\"give me the figure of diamond dimensions\"})\n",
    "print(response['answer'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
